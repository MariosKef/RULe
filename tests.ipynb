{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pygmo as pg\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = pd.read_csv('./results_no_cv_HO_29_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pareto_index = pg.non_dominated_front_2d(configs[['rmse_test', 'std_test']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pareto_configs = configs.iloc[pareto_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pareto_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pareto_configs.iloc[1].net_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pareto_configs['rmse_test'].values\n",
    "y = pareto_configs['std_test'].values\n",
    "plt.fig\n",
    "plt.scatter(x, y)\n",
    "plt.xlabel('rmse_test')\n",
    "plt.ylabel('std_test')\n",
    "\n",
    "# for i, txt in enumerate(pareto_configs.index):\n",
    "#     plt.annotate(txt, (x[i], y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pareto_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme()\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.scatterplot(data=pareto_configs, x='rmse_test', y='std_test')\n",
    "plt.title('Pareto Front')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pareto_configs.loc[186].net_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import check_random_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_random_state(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "np.random.choice(range(1,100), replace=False, size=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [14, 20, 51, 60, 71, 74, 82, 86, 91, 92]\n",
    "train = [x for x in range(0,100) if x not in test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.get_state(np.random.seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from preprocessing import build_data\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from activations import Activate\n",
    "from losses import CustomLoss\n",
    "from preprocessing import build_data\n",
    "\n",
    "\n",
    "import main\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weibull_mean(alpha, beta):\n",
    "    return alpha * math.gamma(1 + 1/beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kefalasm/Projects/rul_sa/mipego/mipego.py:215: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  self._levels = np.array([self._space.bounds[i] for i in self._space.id_N]) # levels for discrete variable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_time': 100, 'lr': '0.001', 'num_rec': 3, 'neuron_0': 100, 'activation_0': 'tanh', 'dropout_0': 0.25, 'recurrent_dropout_0': 0.25, 'neuron_1': 50, 'activation_1': 'tanh', 'dropout_1': 0.25, 'recurrent_dropout_1': 0.25, 'neuron_2': 20, 'activation_2': 'tanh', 'dropout_2': 0.25, 'recurrent_dropout_2': 0.25, 'final_activation_0': 'exp', 'final_activation_1': 'softplus', 'percentage': 50, 'rul': 115, 'rul_style': 'nonlinear', 'batch': '128'}\n",
      "(16340, 26)\n",
      "(10312, 26)\n",
      "(100,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:00<00:00, 83.46it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 12591.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x (16340, 100, 17) train_y (16340, 1) test_x (100, 100, 17) test_y (100, 1)\n",
      "WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"weibull_params\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 100, 17)]         0         \n",
      "_________________________________________________________________\n",
      "masking (Masking)            (None, 100, 17)           0         \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 100, 100)          35700     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 100, 50)           22800     \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 20)                4320      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 42        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 62,862\n",
      "Trainable params: 62,862\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kefalasm/rul_nn/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "128/128 [==============================] - 119s 889ms/step - loss: 2.0580 - val_loss: 0.2433\n",
      "Epoch 2/5\n",
      "128/128 [==============================] - 113s 884ms/step - loss: 0.2163 - val_loss: -0.0899\n",
      "Epoch 3/5\n",
      "128/128 [==============================] - 114s 892ms/step - loss: -0.1457 - val_loss: -0.4530\n",
      "Epoch 4/5\n",
      "128/128 [==============================] - 114s 893ms/step - loss: -0.4130 - val_loss: -0.5514\n",
      "Epoch 5/5\n",
      "128/128 [==============================] - 115s 901ms/step - loss: -0.5697 - val_loss: -0.6276\n"
     ]
    }
   ],
   "source": [
    "# model, train_results_df, test_results_df, test_x_orig, test_y_orig, scaler, train_x, test_x, total = main.main()\n",
    "model = main.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.functional.Functional at 0x7fb91006e340>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('./toy_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_model = tf.keras.models.load_model('./toy_model', custom_objects={\"CustomLoss\": CustomLoss, \"Activate\": Activate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"  #\"1,2,3,4,5\"  # uncomment in case running ONLY on CPU is required\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "from tensorflow.keras import backend as k\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.callbacks import History\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from activations import Activate\n",
    "from losses import CustomLoss\n",
    "from preprocessing import build_data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn import pipeline\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_cfg = {\n",
    "     \"max_time\": 100,\n",
    "     \"lr\": \"0.001\",\n",
    "     \"num_rec\": 3,\n",
    "     \"neuron_0\": 100,\n",
    "     \"activation_0\": \"tanh\",\n",
    "     \"dropout_0\": 0.25,\n",
    "     \"recurrent_dropout_0\": 0.25,\n",
    "     \"neuron_1\": 50,\n",
    "     \"activation_1\": \"tanh\",\n",
    "     \"dropout_1\": 0.25,\n",
    "     \"recurrent_dropout_1\": 0.25,\n",
    "     \"neuron_2\": 20,\n",
    "     \"activation_2\": \"tanh\",\n",
    "     \"dropout_2\": 0.25,\n",
    "     \"recurrent_dropout_2\": 0.25,\n",
    "     \"final_activation_0\": \"exp\",\n",
    "     \"final_activation_1\": \"softplus\",\n",
    "     \"percentage\": 50,\n",
    "     \"rul\": 115,\n",
    "     \"rul_style\": \"nonlinear\",\n",
    "     \"batch\": \"128\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "        \"cv\": 10,\n",
    "        \"shuffle\": True,\n",
    "        \"random_state\": 21,\n",
    "        \"mask_value\": -99,\n",
    "        \"reps\": 30,\n",
    "        \"epochs\": 5,\n",
    "        \"batches\": 64,\n",
    "        \"in_reps\": 10,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weibull_mean(alpha, beta):\n",
    "    return alpha * math.gamma(1 + 1/beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network(train_X, train_y, net_cfg, cfg):\n",
    "    k.set_epsilon(1e-10)\n",
    "    history = History()\n",
    "    nan_terminator = callbacks.TerminateOnNaN()\n",
    "    reduce_lr = callbacks.ReduceLROnPlateau(monitor='loss')\n",
    "    early_stopping = callbacks.EarlyStopping(monitor='loss', patience=5)\n",
    "    # checkpoint_filepath = './saved_models/cp-{epoch:04d}.ckpt'\n",
    "    # checkpoint = callbacks.ModelCheckpoint(filepath=checkpoint_filepath, monitor='loss', verbose=1)\n",
    "    # logdir=\"logs/test1\" #+ datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    # tensorboard = callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "    window = train_X.shape[1]\n",
    "    n_features = train_X.shape[2]\n",
    "\n",
    "    inputs = keras.Input(shape=(window, n_features))\n",
    "    masking_layer = keras.layers.Masking(mask_value=cfg['mask_value'])(inputs)\n",
    "\n",
    "    # recurrent layers\n",
    "    if net_cfg['num_rec'] > 1:\n",
    "        for i in np.arange(net_cfg['num_rec']-1):\n",
    "            masking_layer = keras.layers.GRU(net_cfg['neuron_'+str(i)], activation=net_cfg['activation_'+str(i)],\n",
    "                                    dropout=net_cfg['dropout_'+str(i)],\n",
    "                                    recurrent_dropout=net_cfg['recurrent_dropout_'+str(i)],\n",
    "                                    return_sequences=True)(masking_layer)\n",
    "    last = i + 1\n",
    "    gru_last = keras.layers.GRU(net_cfg['neuron_'+str(last)], activation=net_cfg['activation_'+str(last)],\n",
    "                                dropout=net_cfg['dropout_'+str(last)],\n",
    "                                recurrent_dropout=net_cfg['recurrent_dropout_'+str(last)],\n",
    "                                return_sequences=False)(masking_layer)\n",
    "\n",
    "    dense_1 = keras.layers.Dense(2)(gru_last)\n",
    "    custom_activation = Activate(net_cfg=net_cfg)\n",
    "    outputs = keras.layers.Activation(custom_activation)(dense_1)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"weibull_params\")\n",
    "\n",
    "    # rmse = tf.keras.metrics.RootMeanSquaredError()\n",
    "    model.compile(loss=CustomLoss(kind='continuous', reduce_loss=True), optimizer=Adam(lr=eval(net_cfg['lr']),\n",
    "                                                                                       clipvalue=0.5))\n",
    "    model.summary()  # uncomment for debugging\n",
    "\n",
    "    batch_size=eval(net_cfg['batch'])\n",
    "    model.fit(train_X, train_y,\n",
    "              epochs=cfg['epochs'],\n",
    "              batch_size=eval(net_cfg['batch']),\n",
    "              verbose=1,\n",
    "              callbacks=[nan_terminator, history, reduce_lr, early_stopping],#, checkpoint, tensorboard],  # , tensorboard\n",
    "              workers=32)\n",
    "\n",
    "    return model, history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "\n",
    "    np.random.seed(42)\n",
    "\n",
    "    id_col = 'unit_number'\n",
    "    time_col = 'time'\n",
    "    feature_cols = ['op_setting_1', 'op_setting_2', 'op_setting_3'] + ['sensor_measurement_{}'.format(x) for x in\n",
    "                                                                       range(1, 22)]\n",
    "    column_names = [id_col, time_col] + feature_cols\n",
    "\n",
    "    train_x_orig = pd.read_csv('./DataSets/CMAPSS/train_FD001.csv', header=None, sep='\\s+', decimal=\".\")\n",
    "    train_x_orig.columns = column_names\n",
    "\n",
    "    test_x_orig = pd.read_csv('./DataSets/CMAPSS/test_FD001.csv', header=None, sep='\\s+', decimal=\".\")\n",
    "    test_x_orig.columns = column_names\n",
    "\n",
    "    test_y_orig = pd.read_csv('./DataSets/CMAPSS/RUL_FD001.csv', header=None, names=['T'])\n",
    "\n",
    "    # Make engine numbers and days zero-indexed\n",
    "    train_x_orig.iloc[:, 0:2] -= 1\n",
    "    test_x_orig.iloc[:, 0:2] -= 1\n",
    "\n",
    "    train_idx = np.random.choice(range(train_x_orig.unit_number.unique().max()+1), replace=False, size=80)  # selecting 80 units for training\n",
    "    train_idx.sort()\n",
    "\n",
    "    vld_idx = np.array([x for x in range(train_x_orig.unit_number.unique().max()+1) if x not in train_idx])  # remaining are validation indices\n",
    "\n",
    "    train_or = train_x_orig[train_x_orig.unit_number.isin(train_idx)]  # training data\n",
    "    train_or.reset_index(drop=True, inplace=True)\n",
    "    vld = train_x_orig[train_x_orig.unit_number.isin(vld_idx)]  # validation data\n",
    "    vld.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "    # Truncating the validation data randomly 5 times each\n",
    "    vld_trunc = []\n",
    "    test_y = []\n",
    "    max_cycle = []\n",
    "    test_index = []  # for debugging purposes\n",
    "    temp_or_test_cycles = []\n",
    "    counter = -1\n",
    "\n",
    "    for i in set(vld.unit_number.unique()):\n",
    "        # print(f'unit number is {i}')\n",
    "        for j in range(1,6):  # 5 truncations per instance\n",
    "            counter += 1\n",
    "            np.random.seed(i*j)\n",
    "            temp_df = vld[vld.unit_number == i]\n",
    "            temp_df.reset_index(drop=True, inplace=True)  # important\n",
    "            length = temp_df.shape[0]\n",
    "            # print(length)\n",
    "            temp_or_test_cycles.append(length)\n",
    "            level = np.random.choice(np.arange(5, 96), 1)[0]\n",
    "            r = np.int(length * (1 - level / 100))\n",
    "            # test_index.append(X_test_or[X_test_or.unit_number == i].index.tolist()[\n",
    "            #                     :r + 1])  # check this with train_x_orig instead of X_test_or (probably it's the same)\n",
    "            temp_df = temp_df.truncate(after=r)\n",
    "            # print(temp_df.shape[0])\n",
    "            # print('\\n')\n",
    "            temp_df['unit_number'] = np.repeat(counter, temp_df.shape[0])\n",
    "            vld_trunc.append(temp_df)\n",
    "            max_cycle.append(length)\n",
    "\n",
    "\n",
    "    # test_index = [item for sublist in test_index for item in sublist]\n",
    "\n",
    "    vld_trunc = pd.concat(vld_trunc)\n",
    "    vld_trunc.reset_index(drop=True, inplace=True)\n",
    "    # print(f'max len per unit is {max_cycle}')\n",
    "\n",
    "\n",
    "    # Pre-processing data\n",
    "    scaler = pipeline.Pipeline(steps=[\n",
    "        ('minmax', MinMaxScaler(feature_range=(-1, 1))),\n",
    "        ('remove_constant', VarianceThreshold())])\n",
    "\n",
    "    train = train_or.copy()\n",
    "    train = np.concatenate([train[['unit_number', 'time']], scaler.fit_transform(train[feature_cols])], axis=1)\n",
    "\n",
    "    train_x, train_y = build_data(units=train[:, 0], time=train[:, 1], x=train[:, 2:], max_time=net_cfg['max_time'],\n",
    "                                    is_test=False, mask_value=cfg['mask_value'],\n",
    "                                    original_data=None, net_cfg = net_cfg, label=net_cfg['rul_style'])\n",
    "\n",
    "    vld = vld_trunc.copy()\n",
    "    vld = np.concatenate([vld[['unit_number', 'time']], scaler.transform(vld[feature_cols])], axis=1)\n",
    "    test_x, test_y = build_data(units=vld[:, 0],\n",
    "        time=vld[:, 1],\n",
    "        x=vld[:, 2:],\n",
    "        max_time=net_cfg[\"max_time\"],\n",
    "        is_test=True,\n",
    "        mask_value=cfg[\"mask_value\"],\n",
    "        original_data=np.array(max_cycle),\n",
    "        net_cfg=net_cfg,\n",
    "        label=net_cfg[\"rul_style\"],\n",
    "    )\n",
    "\n",
    "    return train_x, train_y, test_x, test_y, train_or, vld_trunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:00<00:00, 100.57it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 18112.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x (16340, 100, 17) train_y (16340, 1) test_x (100, 100, 17) test_y (100, 1) train (16340, 26) test (10312, 26)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, test_x, test_y, train, test = load_data()\n",
    "print('train_x', train_x.shape, 'train_y', train_y.shape, 'test_x', test_x.shape, 'test_y', test_y.shape, 'train', train.shape, 'test', test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kefalasm/rul_nn/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"weibull_params\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 100, 17)]         0         \n",
      "_________________________________________________________________\n",
      "masking (Masking)            (None, 100, 17)           0         \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 100, 100)          35700     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 100, 50)           22800     \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 20)                4320      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 42        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 62,862\n",
      "Trainable params: 62,862\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "128/128 [==============================] - 54s 384ms/step - loss: 2.5260\n",
      "\n",
      "Epoch 00001: saving model to ./saved_models/cp-0001.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as activate_layer_call_and_return_conditional_losses, activate_layer_call_fn, activate_layer_call_fn, activate_layer_call_and_return_conditional_losses, activate_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_models/cp-0001.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_models/cp-0001.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "128/128 [==============================] - 48s 374ms/step - loss: 0.2623\n",
      "\n",
      "Epoch 00002: saving model to ./saved_models/cp-0002.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as activate_layer_call_and_return_conditional_losses, activate_layer_call_fn, activate_layer_call_fn, activate_layer_call_and_return_conditional_losses, activate_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_models/cp-0002.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_models/cp-0002.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      " 40/128 [========>.....................] - ETA: 32s - loss: 0.0204"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-068c893899ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet_cfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-4c9ce3c35f41>\u001b[0m in \u001b[0;36mnetwork\u001b[0;34m(train_X, train_y, net_cfg, cfg)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_cfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     model.fit(train_X, train_y,\n\u001b[0m\u001b[1;32m     44\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m               \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_cfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rul_nn/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rul_nn/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rul_nn/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rul_nn/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3021\u001b[0m       (graph_function,\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3023\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rul_nn/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1960\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/rul_nn/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rul_nn/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "k.clear_session()\n",
    "try:\n",
    "    del model\n",
    "except:\n",
    "    pass\n",
    "\n",
    "model, history = network(train_x, train_y, net_cfg, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(cfg[\"reps\"]):\n",
    "        tf.random.set_seed(i)\n",
    "        train_predict = model(train_x, training=True).numpy()\n",
    "        a, b = train_predict[:, 0], train_predict[:, 1]\n",
    "\n",
    "        res = []\n",
    "        for j in range(a.shape[0]):\n",
    "            res.append(a[j] * np.random.weibull(b[j], cfg[\"in_reps\"]))\n",
    "        total_res_train.append(res)\n",
    "    total_res_train = np.array(total_res_train)\n",
    "\n",
    "    train_predict = np.mean(total_res_train, axis=(0, 2))\n",
    "    train_predict = np.reshape(train_predict, (train_x.shape[0], 1))\n",
    "    train_result = np.concatenate((train_y, train_predict), axis=1)\n",
    "    train_results_df = pd.DataFrame(train_result, columns=[\"T\", \"predicted_mu\"])\n",
    "    train_results_df[\"unit_number\"] = train_x_orig[\"unit_number\"].to_numpy()\n",
    "    train_results_df[\"time\"] = train_x_orig[\"time\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "for i in range(train_x.shape[0]):\n",
    "            tf.random.set_seed(i)\n",
    "            train_predict = model(train_x[i].reshape(1, train_x[i].shape[0], train_x[i].shape[1]), training=True).numpy()\n",
    "            a, b = train_predict[:,0], train_predict[:,1]\n",
    "            samples.append(b.reshape(-1,1)*np.random.weibull(a.reshape(-1,1), size=(a.shape[0], 30)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model.history.history['loss'],    label='training')\n",
    "plt.plot(model.history.history['val_loss'],label='validation')\n",
    "plt.title('loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weibull_predictions(results_df, unit):\n",
    "\n",
    "#     fig, axarr = plt.subplots(3, figsize=(30,20))\n",
    "    \n",
    "    plt.figure(figsize=(30,10))\n",
    "\n",
    "    t=np.arange(0,400)\n",
    "\n",
    "    palette = sns.color_palette(\"RdBu_r\", results_df.shape[0] + 1)\n",
    "    color_dict = dict(enumerate(palette))\n",
    "\n",
    "#     ax=axarr[0]\n",
    "    \n",
    "#     median_predictions = weibull_median(results_df['alpha'], results_df['beta'])\n",
    "    mean_predictions =  results_df['predicted_mu'] # results_df[['alpha', 'beta']].apply(lambda row: weibull_mean(row[0], row[1]), axis=1)\n",
    "    std_plus = results_df['predicted_std+']\n",
    "    std_minus = results_df['predicted_std-']\n",
    "\n",
    "    x = results_df['time']\n",
    "    \n",
    "    plt.plot(x, results_df['T'], label='survival_time', color='black')\n",
    "\n",
    "#     ax.plot(x, median_predictions, label='median_prediction')\n",
    "    plt.plot(x, mean_predictions, label='mean_prediction')\n",
    "#     ax.plot(x, mode_predictions, label='mode_prediction')\n",
    "#     ax.set_title('MAP prediction Vs. true')\n",
    "    plt.plot(x, std_plus, 'g-')\n",
    "    plt.plot(x, std_minus, 'g-')\n",
    "    \n",
    "\n",
    "    plt.legend()\n",
    "    \n",
    "#     ax=axarr[1]\n",
    "#     sns.distplot(results_df['T'] - mode_predictions, ax=ax)\n",
    "#     ax.set_title('Error')\n",
    "\n",
    "#     ax.plot(x, results_df['alpha'], label='alpha')\n",
    "#     ax.legend()\n",
    "    \n",
    "    \n",
    "#     ax = axarr[2]\n",
    "#     ax.plot(x, results_df['beta'], label='beta')\n",
    "#     ax.legend()\n",
    "    \n",
    "#     ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.suptitle(unit)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_df['T'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_cfg = {\"num_rec\": 2, \"max_time\": 25, \"neuron_0\": 74, \"neuron_1\": 71, \"neuron_2\": 78, \"neuron_3\": 68,\n",
    "     \"activation_0\": \"tanh\", \"activation_1\": \"sigmoid\", \"activation_2\": \"sigmoid\", \"activation_3\": \"tanh\",\n",
    "      \"dropout_0\": 0.04198965650706104, \"dropout_1\": 0.6518949855946009, \"dropout_2\": 0.5134433415117658,\n",
    "       \"dropout_3\": 0.44181882048621723, \"recurrent_dropout_0\": 0.011379281378212352, \"recurrent_dropout_1\": 0.07877843876273939,\n",
    "        \"recurrent_dropout_2\": 0.25685072170110057, \"recurrent_dropout_3\": 0.16640448683710898, \"final_activation_0\": \"softplus\",\n",
    "         \"final_activation_1\": \"softplus\", \"percentage\": 63, \"rul\": 125, \"rul_style\": \"nonlinear\", \"lr\": 0.021745779733326226, \"batch\": \"256\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_col = 'unit_number'\n",
    "time_col = 'time'\n",
    "max_time = 32\n",
    "mask_value = -99\n",
    "\n",
    "feature_cols = ['op_setting_1', 'op_setting_2', 'op_setting_3'] + ['sensor_measurement_{}'.format(x) for x in\n",
    "                                                                    range(1, 22)]\n",
    "column_names = [id_col, time_col] + feature_cols\n",
    "test_or = test_x_orig.copy()\n",
    "test_or = np.concatenate([test_or[['unit_number', 'time']], scaler.transform(test_or[feature_cols])], axis=1)\n",
    "\n",
    "\n",
    "# Preparing data for the RNN (numpy arrays)\n",
    "test_or, _ = build_data(units=test_or[:, 0], time=test_or[:, 1], x=test_or[:, 2:], max_time=net_cfg['max_time'],\n",
    "                                is_test=True, mask_value=-99,\n",
    "                                original_data=np.repeat(200, test_or.shape[0]), net_cfg = net_cfg, label=net_cfg['rul_style'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps = 30\n",
    "test_predict_1_or = []\n",
    "test_predict_2_or = []\n",
    "for _ in range(reps):\n",
    "    test_predict_or = model(test_or, training=True).numpy()\n",
    "#     print(test_predict_or.shape)\n",
    "#     break\n",
    "    test_predict_1_or.append(test_predict_or[:,0].reshape(test_predict_or[:,0].shape[0], 1))\n",
    "    test_predict_2_or.append(test_predict_or[:,1].reshape(test_predict_or[:,1].shape[0], 1))\n",
    "\n",
    "test_predict_1_mean_or = np.average(np.hstack(test_predict_1_or), axis=1)\n",
    "test_predict_2_mean_or = np.average(np.hstack(test_predict_2_or), axis=1)\n",
    "test_predict_1_mean_or = test_predict_1_mean_or.reshape(test_predict_1_mean_or.shape[0],1)\n",
    "test_predict_2_mean_or = test_predict_2_mean_or.reshape(test_predict_2_mean_or.shape[0],1)\n",
    "test_predict_1_std_or = np.std(np.hstack(test_predict_1_or), axis=1)\n",
    "test_predict_2_std_or = np.std(np.hstack(test_predict_2_or), axis=1)\n",
    "test_predict_1_std_or = test_predict_1_std_or.reshape(test_predict_1_std_or.shape[0],1)\n",
    "test_predict_2_std_or = test_predict_2_std_or.reshape(test_predict_2_std_or.shape[0],1)\n",
    "\n",
    "\n",
    "test_predict_or = np.hstack([test_predict_1_mean_or, test_predict_2_mean_or, \n",
    "                           test_predict_1_std_or, test_predict_2_std_or])    \n",
    "\n",
    "test_predict_or = np.resize(test_predict_or, (test_or.shape[0], 4))  # changed from 2 to 4\n",
    "test_result_or = np.concatenate((test_y_orig, test_predict_or), axis=1)\n",
    "test_results_df_or = pd.DataFrame(test_result_or, columns=['T',   'mean_alpha', 'mean_beta', 'std_alpha', 'std_beta']) # (add 'E' for event)\n",
    "#     test_results_df['unit_number'] = train_x_orig.iloc[test_index]['unit_number'].to_numpy() # Note the train_x_orig\n",
    "#     test_results_df['time'] = train_x_orig.iloc[test_index]['time'].to_numpy()\n",
    "\n",
    "test_results_df_or['predicted_mu'] = test_results_df_or[['mean_alpha', 'mean_beta']].apply(lambda row: weibull_mean(row[0], row[1]), axis=1)\n",
    "test_results_df_or['predicted_std+'] = test_results_df_or[['mean_alpha', 'mean_beta', 'std_alpha', \n",
    "                                                       'std_beta']].apply(lambda row: weibull_mean(row[0]+1.96*row[2]/np.sqrt(reps), \n",
    "                                                                                                   row[1]+1.96*row[3]/np.sqrt(reps)), axis=1)\n",
    "test_results_df_or['predicted_std-'] = test_results_df_or[['mean_alpha', 'mean_beta', 'std_alpha', \n",
    "                                                           'std_beta']].apply(lambda row: weibull_mean(row[0]-1.96*row[2]/np.sqrt(reps),\n",
    "                                                                                                       row[1]-1.96*row[3]/np.sqrt(reps)), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Old way\n",
    "# test_predict_or = model(test_or, training=True).numpy()  # equivalent to model.predict(test_or) but with training=False\n",
    "# test_predict_or = np.resize(test_predict_or, (test_or.shape[0], 2))\n",
    "# test_result_or = np.concatenate((test_y_orig, test_predict_or), axis=1)\n",
    "# test_results_df_or = pd.DataFrame(test_result_or, columns=['T', 'alpha', 'beta'])\n",
    "# test_results_df_or['unit_number'] = np.arange(1, test_results_df_or.shape[0]+1)\n",
    "\n",
    "# test_results_df_or['predicted_mu'] = test_results_df_or[['alpha', 'beta']].apply(lambda row: weibull_mean(row[0], row[1]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_df_or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 - 10 epochs\n",
    "np.sqrt(mean_squared_error(test_results_df_or['predicted_mu'], test_results_df_or['T']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 - 50 epochs\n",
    "np.sqrt(mean_squared_error(test_results_df_or['predicted_mu'], test_results_df_or['T']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 - 21/50 epochs\n",
    "np.sqrt(mean_squared_error(test_results_df_or['predicted_mu'], test_results_df_or['T']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for unit_number, grp in train_results_df.groupby('unit_number'):\n",
    "    plot_weibull_predictions(grp, unit_number)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for unit_number, grp in test_results_df.groupby('unit_number'):\n",
    "    plot_weibull_predictions(grp, unit_number)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training full model on the best HP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2,3,4,5\"  # uncomment in case running ONLY on CPU is required\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "from tensorflow.keras import backend as k\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.callbacks import History\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from activations import Activate\n",
    "from losses import CustomLoss\n",
    "from preprocessing import build_data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn import pipeline\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_cfg = {\"num_rec\": 3, \"max_time\": 26, \"neuron_0\": 73, \"neuron_1\": 71, \"neuron_2\": 82, \"neuron_3\": 82, \"activation_0\": \"tanh\",\n",
    "     \"activation_1\": \"sigmoid\", \"activation_2\": \"sigmoid\", \"activation_3\": \"tanh\", \"dropout_0\": 0.06943171652267692,\n",
    "      \"dropout_1\": 0.12639579059484615, \"dropout_2\": 0.3822443511564662, \"dropout_3\": 0.4580962846531429,\n",
    "       \"recurrent_dropout_0\": 0.3280089650917844, \"recurrent_dropout_1\": 0.69930466502713, \"recurrent_dropout_2\": 0.24506744915217923,\n",
    "        \"recurrent_dropout_3\": 0.7699919737017498, \"final_activation_0\": \"exp\", \"final_activation_1\": \"softplus\", \"percentage\": 73,\n",
    "        \"rul\": 121, \"rul_style\": \"nonlinear\", \"lr\": 0.005357912753227542, \"batch\": \"128\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {'cv': 10, 'shuffle': True,\n",
    "'random_state': 21,\n",
    "'mask_value': -99,\n",
    "'reps': 30,\n",
    "'epochs': 100,\n",
    "'batches': 64}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weibull_mean(alpha, beta):\n",
    "    return alpha * math.gamma(1 + 1/beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network(train_X, train_y, net_cfg, cfg):\n",
    "    k.set_epsilon(1e-10)\n",
    "    history = History()\n",
    "    nan_terminator = callbacks.TerminateOnNaN()\n",
    "    reduce_lr = callbacks.ReduceLROnPlateau(monitor='loss')\n",
    "    early_stopping = callbacks.EarlyStopping(monitor='loss', patience=5)\n",
    "    checkpoint_filepath = './saved_models/cp-{epoch:04d}.ckpt'\n",
    "    checkpoint = callbacks.ModelCheckpoint(filepath=checkpoint_filepath, monitor='loss', verbose=1)\n",
    "    logdir=\"logs/test1\" #+ datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard = callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "    window = train_X.shape[1]\n",
    "    n_features = train_X.shape[2]\n",
    "\n",
    "    inputs = keras.Input(shape=(window, n_features))\n",
    "    masking_layer = keras.layers.Masking(mask_value=cfg['mask_value'])(inputs)\n",
    "\n",
    "    # recurrent layers\n",
    "    if net_cfg['num_rec'] > 1:\n",
    "        for i in np.arange(net_cfg['num_rec']-1):\n",
    "            masking_layer = keras.layers.GRU(net_cfg['neuron_'+str(i)], activation=net_cfg['activation_'+str(i)],\n",
    "                                    dropout=net_cfg['dropout_'+str(i)],\n",
    "                                    recurrent_dropout=net_cfg['recurrent_dropout_'+str(i)],\n",
    "                                    return_sequences=True)(masking_layer)\n",
    "    last = i + 1\n",
    "    gru_last = keras.layers.GRU(net_cfg['neuron_'+str(last)], activation=net_cfg['activation_'+str(last)],\n",
    "                                dropout=net_cfg['dropout_'+str(last)],\n",
    "                                recurrent_dropout=net_cfg['recurrent_dropout_'+str(last)],\n",
    "                                return_sequences=False)(masking_layer)\n",
    "\n",
    "    dense_1 = keras.layers.Dense(2)(gru_last)\n",
    "    custom_activation = Activate(net_cfg=net_cfg)\n",
    "    outputs = keras.layers.Activation(custom_activation)(dense_1)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"weibull_params\")\n",
    "\n",
    "    # rmse = tf.keras.metrics.RootMeanSquaredError()\n",
    "    model.compile(loss=CustomLoss(kind='continuous', reduce_loss=True), optimizer=Adam(lr=net_cfg['lr'],\n",
    "                                                                                       clipvalue=0.5))\n",
    "    model.summary()  # uncomment for debugging\n",
    "\n",
    "    batch_size=eval(net_cfg['batch'])\n",
    "    model.fit(train_X, train_y,\n",
    "              epochs=cfg['epochs'],\n",
    "              batch_size=eval(net_cfg['batch']),\n",
    "              verbose=1,\n",
    "              callbacks=[nan_terminator, history, reduce_lr, early_stopping, checkpoint],#, tensorboard],  # , tensorboard\n",
    "              workers=32)\n",
    "\n",
    "    return model, history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir=logdir --port 8889\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "\n",
    "    np.random.seed(42)\n",
    "\n",
    "    id_col = 'unit_number'\n",
    "    time_col = 'time'\n",
    "    feature_cols = ['op_setting_1', 'op_setting_2', 'op_setting_3'] + ['sensor_measurement_{}'.format(x) for x in\n",
    "                                                                       range(1, 22)]\n",
    "    column_names = [id_col, time_col] + feature_cols\n",
    "\n",
    "    train_x_orig = pd.read_csv('./DataSets/CMAPSS/train_FD001.csv', header=None, sep='\\s+', decimal=\".\")\n",
    "    train_x_orig.columns = column_names\n",
    "\n",
    "    test_x_orig = pd.read_csv('./DataSets/CMAPSS/test_FD001.csv', header=None, sep='\\s+', decimal=\".\")\n",
    "    test_x_orig.columns = column_names\n",
    "\n",
    "    test_y_orig = pd.read_csv('./DataSets/CMAPSS/RUL_FD001.csv', header=None, names=['T'])\n",
    "\n",
    "    # Make engine numbers and days zero-indexed\n",
    "    train_x_orig.iloc[:, 0:2] -= 1\n",
    "    test_x_orig.iloc[:, 0:2] -= 1\n",
    "\n",
    "    # Pre-processing data\n",
    "    scaler = pipeline.Pipeline(steps=[\n",
    "        ('minmax', MinMaxScaler(feature_range=(-1, 1))),\n",
    "        ('remove_constant', VarianceThreshold())])\n",
    "\n",
    "    train = train_x_orig.copy()\n",
    "    train = np.concatenate([train[['unit_number', 'time']], scaler.fit_transform(train[feature_cols])], axis=1)\n",
    "\n",
    "    train_x, train_y = build_data(units=train[:, 0], time=train[:, 1], x=train[:, 2:], max_time=net_cfg['max_time'],\n",
    "                                    is_test=False, mask_value=cfg['mask_value'],\n",
    "                                    original_data=None, net_cfg = net_cfg, label=net_cfg['rul_style'])\n",
    "\n",
    "    test_or = test_x_orig.copy()\n",
    "    test_or = np.concatenate([test_or[['unit_number', 'time']], scaler.transform(test_or[feature_cols])], axis=1)\n",
    "\n",
    "\n",
    "    # Preparing data for the RNN (numpy arrays)\n",
    "    test_or, _ = build_data(units=test_or[:, 0], time=test_or[:, 1], x=test_or[:, 2:], max_time=net_cfg['max_time'],\n",
    "                                    is_test=True, mask_value=-99,\n",
    "                                    original_data=np.repeat(200, test_or.shape[0]), net_cfg = net_cfg, label=net_cfg['rul_style'])\n",
    "\n",
    "    \n",
    "\n",
    "    return train_x, train_y, test_or, test_y_orig, train_x_orig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y, test_x, test_y, train_x_orig = load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train_x', train_x.shape, 'train_y', train_y.shape, 'test_x', test_x.shape, 'test_y', test_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k.clear_session()\n",
    "try:\n",
    "    del model\n",
    "except:\n",
    "    pass\n",
    "\n",
    "model, history = network(train_x, train_y, net_cfg, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('../autokeras_rul/best_model_autokeras_with_vld') #, custom_objects={\"CustomLoss\": CustomLoss, \"Activate\": Activate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model.history.history['loss'],    label='training')\n",
    "plt.title('loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps = 30\n",
    "test_predict_1_or = []\n",
    "test_predict_2_or = []\n",
    "for _ in range(reps):\n",
    "    test_predict_or = model(test_x, training=True).numpy()\n",
    "#     print(test_predict_or.shape)\n",
    "#     break\n",
    "    test_predict_1_or.append(test_predict_or[:,0].reshape(test_predict_or[:,0].shape[0], 1))\n",
    "    test_predict_2_or.append(test_predict_or[:,1].reshape(test_predict_or[:,1].shape[0], 1))\n",
    "\n",
    "test_predict_1_mean_or = np.average(np.hstack(test_predict_1_or), axis=1)\n",
    "test_predict_2_mean_or = np.average(np.hstack(test_predict_2_or), axis=1)\n",
    "test_predict_1_mean_or = test_predict_1_mean_or.reshape(test_predict_1_mean_or.shape[0],1)\n",
    "test_predict_2_mean_or = test_predict_2_mean_or.reshape(test_predict_2_mean_or.shape[0],1)\n",
    "test_predict_1_std_or = np.std(np.hstack(test_predict_1_or), axis=1)\n",
    "test_predict_2_std_or = np.std(np.hstack(test_predict_2_or), axis=1)\n",
    "test_predict_1_std_or = test_predict_1_std_or.reshape(test_predict_1_std_or.shape[0],1)\n",
    "test_predict_2_std_or = test_predict_2_std_or.reshape(test_predict_2_std_or.shape[0],1)\n",
    "\n",
    "\n",
    "test_predict_or = np.hstack([test_predict_1_mean_or, test_predict_2_mean_or, \n",
    "                           test_predict_1_std_or, test_predict_2_std_or])    \n",
    "\n",
    "test_predict_or = np.resize(test_predict_or, (test_x.shape[0], 4))  # changed from 2 to 4\n",
    "test_result_or = np.concatenate((test_y, test_predict_or), axis=1)\n",
    "test_results_df_or = pd.DataFrame(test_result_or, columns=['T',   'mean_alpha', 'mean_beta', 'std_alpha', 'std_beta']) # (add 'E' for event)\n",
    "#     test_results_df['unit_number'] = train_x_orig.iloc[test_index]['unit_number'].to_numpy() # Note the train_x_orig\n",
    "#     test_results_df['time'] = train_x_orig.iloc[test_index]['time'].to_numpy()\n",
    "\n",
    "test_results_df_or['predicted_mu'] = test_results_df_or[['mean_alpha', 'mean_beta']].apply(lambda row: weibull_mean(row[0], row[1]), axis=1)\n",
    "# test_results_df_or['predicted_std+'] = test_results_df_or[['mean_alpha', 'mean_beta', 'std_alpha', \n",
    "#                                                        'std_beta']].apply(lambda row: weibull_mean(row[0]+1.96*row[2]/np.sqrt(reps), \n",
    "#                                                                                                    row[1]+1.96*row[3]/np.sqrt(reps)), axis=1)\n",
    "# test_results_df_or['predicted_std-'] = test_results_df_or[['mean_alpha', 'mean_beta', 'std_alpha', \n",
    "#                                                            'std_beta']].apply(lambda row: weibull_mean(row[0]-1.96*row[2]/np.sqrt(reps),\n",
    "#                                                                                                        row[1]-1.96*row[3]/np.sqrt(reps)), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Old way\n",
    "# test_predict_or = model(test_or, training=True).numpy()  # equivalent to model.predict(test_or) but with training=False\n",
    "# test_predict_or = np.resize(test_predict_or, (test_or.shape[0], 2))\n",
    "# test_result_or = np.concatenate((test_y_orig, test_predict_or), axis=1)\n",
    "# test_results_df_or = pd.DataFrame(test_result_or, columns=['T', 'alpha', 'beta'])\n",
    "# test_results_df_or['unit_number'] = np.arange(1, test_results_df_or.shape[0]+1)\n",
    "\n",
    "# test_results_df_or['predicted_mu'] = test_results_df_or[['alpha', 'beta']].apply(lambda row: weibull_mean(row[0], row[1]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_df_or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predict_1 = []\n",
    "train_predict_2 = []\n",
    "\n",
    "success = True\n",
    "\n",
    "for i in range(cfg['reps']):\n",
    "    tf.random.set_seed(i)\n",
    "    train_predict = model(train_x, training=True).numpy()\n",
    "    train_predict_1.append(train_predict[:, 0].reshape(train_predict[:, 0].shape[0], 1))\n",
    "    train_predict_2.append(train_predict[:, 1].reshape(train_predict[:, 1].shape[0], 1))\n",
    "\n",
    "train_predict_1_mean = np.mean(np.hstack(train_predict_1), axis=1)\n",
    "train_predict_2_mean = np.mean(np.hstack(train_predict_2), axis=1)\n",
    "train_predict_1_mean = train_predict_1_mean.reshape(train_predict_1_mean.shape[0], 1)\n",
    "train_predict_2_mean = train_predict_2_mean.reshape(train_predict_2_mean.shape[0], 1)\n",
    "train_predict_1_std = np.std(np.hstack(train_predict_1), axis=1)\n",
    "train_predict_2_std = np.std(np.hstack(train_predict_2), axis=1)\n",
    "train_predict_1_std = train_predict_1_std.reshape(train_predict_1_std.shape[0], 1)\n",
    "train_predict_2_std = train_predict_2_std.reshape(train_predict_2_std.shape[0], 1)\n",
    "\n",
    "train_predict = np.hstack([train_predict_1_mean, train_predict_2_mean,\n",
    "                            train_predict_1_std, train_predict_2_std])\n",
    "\n",
    "train_predict = np.resize(train_predict, (train_x.shape[0], 4))  # changed from 2 to 4\n",
    "train_result = np.concatenate((train_y, train_predict), axis=1)\n",
    "train_results_df = pd.DataFrame(train_result, columns=['T', 'mean_alpha', 'mean_beta', 'std_alpha',\n",
    "                                                        'std_beta'])  # (add 'E' for event)\n",
    "train_results_df['unit_number'] = train_x_orig['unit_number'].to_numpy()\n",
    "train_results_df['time'] = train_x_orig['time'].to_numpy()\n",
    "\n",
    "train_results_df['predicted_mu'] = train_results_df[['mean_alpha', 'mean_beta']].apply(\n",
    "    lambda row: weibull_mean(row[0], row[1]), axis=1)\n",
    "# train_results_df['predicted_std+'] = train_results_df[['mean_alpha', 'mean_beta', 'std_alpha',\n",
    "#                                                         'std_beta']].apply(\n",
    "#     lambda row: weibull_mean(row[0] + 1.96 * row[2] / np.sqrt(cfg['reps']),\n",
    "#                                 row[1] + 1.96 * row[3] / np.sqrt(cfg['reps'])), axis=1)\n",
    "# train_results_df['predicted_std-'] = train_results_df[['mean_alpha', 'mean_beta', 'std_alpha',\n",
    "#                                                         'std_beta']].apply(\n",
    "#     lambda row: weibull_mean(row[0] - 1.96 * row[2] / np.sqrt(cfg['reps']),\n",
    "#                                 row[1] - 1.96 * row[3] / np.sqrt(cfg['reps'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(mean_squared_error(train_results_df['predicted_mu'], train_results_df['T']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(train_results_df['predicted_mu'], train_results_df['T'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(mean_squared_error(test_results_df_or['predicted_mu'], test_results_df_or['T']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(test_results_df_or['predicted_mu'], test_results_df_or['T'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4000a7a6ee44f5e9ad604830731b987d83f87e046c93fb1fca3b0779acf4e02a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('rul_nn': virtualenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
